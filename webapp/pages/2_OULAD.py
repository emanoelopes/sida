from pathlib import Path
import streamlit as st
import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno
import numpy as np
import pickle
from src.openai_interpreter import criar_rodape_sidebar


st.set_page_config(
    page_title="An√°lise Explorat√≥ria dos Dados - OULAD",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded",
)

#st.markdown('# Informa√ß√µes B√°sicas dos Dados do OULAD')
#st.divider()

datasets_oulad_path = Path(__file__).parent.parents[1] / 'datasets' / 'oulad_data'
#st.write(f"Path dos datasets: {datasets_oulad_path}")

dataframes_oulad = {}

for filename in os.listdir(datasets_oulad_path):
    if filename.endswith('.csv'):
        file_path = os.path.join(datasets_oulad_path, filename)
        df_name = os.path.splitext(filename)[0] # Nome do dataframe ser√° o nome do arquivo sem a extens√£o
        try:
            dataframes_oulad[df_name] = pd.read_csv(file_path, sep=',', encoding='ISO-8859-1')
            print(f"Arquivo '{filename}' carregado com sucesso como dataframe '{df_name}'.")
        except Exception as e:
            print(f"Erro ao carregar o arquivo '{filename}': {e}")


df_assessments = dataframes_oulad['assessments'].head(10_000)
df_courses = dataframes_oulad['courses'].head(10_000)
df_vle = dataframes_oulad['vle'].head(10_000)
df_studentinfo = dataframes_oulad['studentInfo'].head(10_000)
df_studentregistration = dataframes_oulad['studentRegistration'].head(10_000)
df_studentassessment = dataframes_oulad['studentAssessment'].head(10_000)
df_studentvle = dataframes_oulad['studentVle'].head(10_000)

#function to display basic info for a given dataframe
def show_basic_info(df):
    print("========================================================================================================")
    print("HEAD:")
    print(df.head(3))
    print("--------------------------------------------------------------------------------------------------------")
    print("SHAPE:")
    print(df.shape)
    print("--------------------------------------------------------------------------------------------------------")
    print("INFO:")
    print(df.info())
    print("--------------------------------------------------------------------------------------------------------")
    print("DESCRIBE:")
    print(df.describe().T.round(2))
    print("--------------------------------------------------------------------------------------------------------")
    print("NULL VALUES:")
    print(df.isnull().sum())
    print("--------------------------------------------------------------------------------------------------------")
    print("UNIQUE VALUES:")
    print(df.nunique())
    print("--------------------------------------------------------------------------------------------------------")
    print("DUPLICATED VALUES:")
    print(df.duplicated().sum())
    print("--------------------------------------------------------------------------------------------------------")
    print("VALUE COUNTS:")
    print(df.select_dtypes(include=['object']).nunique())
    print("--------------------------------------------------------------------------------------------------------")
    print("========================================================================================================")


# st.sidebar.selectbox('Escolha o dataframe para visualizar informa√ß√µes b√°sicas:', 
#              options=list(dataframes_oulad.keys()),
#              key='selected_oulad_dataframe')

# apresentar o dataframe selecionado
#st.dataframe(dataframes_oulad[st.session_state['selected_oulad_dataframe']])

# # apresentar informa√ß√µes b√°sicas do dataframe selecionado
# st.markdown("### Informa√ß√µes b√°sicas do DataFrame:")
# selected_df = dataframes_oulad[st.session_state['selected_oulad_dataframe']]

# st.markdown("#### Head:")
# st.dataframe(selected_df.head(3))

# st.markdown("#### Shape:")
# st.write(selected_df.shape)

# st.markdown("#### Info:")
# # Capture the output of info() to a string
# import io
# buffer = io.StringIO()
# selected_df.info(buf=buffer)
# info_string = buffer.getvalue()

# # Display the string in a st.code block for better formatting
# st.code(info_string, language='text')

# st.markdown("#### Describe:")
# st.dataframe(selected_df.describe().T.round(2))

# st.markdown("#### Null Values:")
# st.write(selected_df.isnull().sum())

# st.markdown("#### Unique Values:")
# st.write(selected_df.nunique())

# st.markdown("#### Duplicated Values:")
# st.write(selected_df.duplicated().sum())

# st.markdown("#### Value Counts (Object Columns):")
# st.write(selected_df.select_dtypes(include=['object']).nunique())

# # Visualiza√ß√£o de dados faltantes usando missingno
# st.markdown("### Visualiza√ß√£o de Dados Faltantes:")
# st.markdown("#### Matriz de Dados Faltantes:")

# fig, ax = plt.subplots()
# msno.matrix(selected_df, figsize=(6, 4), ax=ax)
# st.pyplot(fig)


new_vle = df_vle.drop(['week_from','week_to'],axis=1)
show_basic_info(new_vle)

# Imputa√ß√£o com os valores mais frequentes por regi√£o
dataframes_oulad['studentInfo']['imd_band_2'] = dataframes_oulad['studentInfo'].apply(lambda x: dataframes_oulad['studentInfo'][dataframes_oulad['studentInfo']['region']==x['region']]['imd_band'].mode()[0] \
    if pd.isna(x['imd_band']) else x['imd_band'], axis=1)
new_studentInfo = df_studentinfo.drop(['imd_band'], axis=1)
show_basic_info(new_studentInfo)

# Imputando valores ausentes em 'date_registration' e 'date_unregistration'
# Criar uma c√≥pia expl√≠cita do dataframe para evitar SettingWithCopyWarning
df_student_registration_copy = df_studentregistration.copy()

# Criar vari√°vel bin√°ria indicando se o estudante cancelou o registro
df_student_registration_copy['cancelou'] = df_student_registration_copy['date_unregistration'].notna().astype(int)

# Preencher date_unregistration com valor alto quando ausente (para diferenciar de valores reais)
# Usar max + 1000 para garantir que seja claramente distinto de qualquer data real
max_date_unregistration = df_student_registration_copy['date_unregistration'].max()
if pd.notna(max_date_unregistration):
    valor_nao_cancelou = max_date_unregistration + 1000
else:
    # Se todos os valores forem NaN, usar um valor padr√£o alto
    valor_nao_cancelou = 999999
df_student_registration_copy['date_unregistration'] = df_student_registration_copy['date_unregistration'].fillna(valor_nao_cancelou)

# Preencher date_registration com a m√©dia quando ausente
mean_date_registration = df_student_registration_copy['date_registration'].mean()
df_student_registration_copy['date_registration'] = df_student_registration_copy['date_registration'].fillna(mean_date_registration)

# Jun√ß√£o dos dados
@st.cache_data(ttl=3600)  # Cache por 1 hora
def merge_dataframes():
    vle_activities = pd.merge(df_studentvle, new_vle, on=['code_module','code_presentation','id_site'], how='inner')
    assessments_activities = pd.merge(df_studentassessment, df_assessments, on='id_assessment', how='inner')
    studentinfo_activities = pd.merge(vle_activities, new_studentInfo, on=['code_module','code_presentation','id_student'], how='inner')
    merged_df = pd.merge(studentinfo_activities, assessments_activities, on=['code_module','code_presentation','id_student'], how='inner')
    return merged_df

merged_df = merge_dataframes()
st.session_state['merged_df'] = merged_df

# Merge with courses dataframe
merged_df = pd.merge(merged_df, df_courses, on=['code_presentation'], how='inner')

# Merge with studentRegistration dataframe (usando a vers√£o processada com vari√°vel cancelou)
merged_df = pd.merge(merged_df, df_student_registration_copy, on=['code_presentation','id_student'], how='inner')

# Imputing missing values for numerical columns with the mean
for col in merged_df.select_dtypes(include=np.number).columns:
    merged_df[col].fillna(merged_df[col].mean(), inplace=True)

# Imputing missing values for categorical columns with the most frequent value
for col in merged_df.select_dtypes(include='object').columns:
    merged_df[col].fillna(merged_df[col].mode()[0], inplace=True)

# st.write("Merged DataFrame after handling missing values:")
# st.dataframe(merged_df.isnull().sum())

# Sidebar com rodap√©
with st.sidebar:
    st.markdown("### üìä Informa√ß√µes")
    st.info("""
    Esta p√°gina apresenta uma an√°lise explorat√≥ria dos dados do OULAD (Open University Learning Analytics Dataset).
    """)
    st.markdown("---")
    # Rodap√© com badges de status (igual ao da home)
    criar_rodape_sidebar()

st.write('# An√°lise Explorat√≥ria de Dados (EDA) - OULAD')

'''
Esta p√°gina apresenta uma An√°lise Explorat√≥ria dos Dados do OULAD (Open University Learning Analytics Dataset), com foco em entender o perfil dos estudantes, suas atividades na plataforma e fatores que influenciam o desempenho acad√™mico . Atrav√©s de visualiza√ß√µes, s√£o identificados padr√µes relevantes, como a predomin√¢ncia de estudantes do g√™nero masculino e a distribui√ß√£o et√°ria dos estudantes.
'''

st.markdown("## Descri√ß√£o estat√≠sticas das colunas num√©ricas:")
st.dataframe(merged_df.select_dtypes('number').describe().T.round(2))

'''
A grande diferen√ßa entre a mediana (‚âà2) e a m√©dia (‚âà4.65) do n√∫mero de cliques indica que a maioria dos estudantes tem engajamento moderado, mas uma pequena parcela √© extremamente ativa, elevando a m√©dia geral.

O n√∫mero de tentativas anteriores √© zero para a vasta maioria dos estudantes (quartis e valor m√°ximo s√£o 0), sugerindo que o conjunto de dados est√° focado na performance na primeira tentativa.
'''


st.write('## Distribui√ß√£o das notas finais dos estudantes')
plt.figure(figsize=(10, 6))
# Calcular nota m√©dia por estudante √∫nico
if 'score' in merged_df.columns and 'id_student' in merged_df.columns:
    notas_por_estudante = merged_df.groupby('id_student')['score'].mean()
    sns.histplot(notas_por_estudante, bins=30, kde=True)
    plt.title('Distribui√ß√£o de Notas Finais dos Estudantes (√önicos)')
    plt.xlabel('Nota Final M√©dia')
    plt.ylabel('N√∫mero de Estudantes √önicos')
else:
    sns.histplot(merged_df['score'], bins=30, kde=True)
    plt.title('Distribui√ß√£o de Notas Finais dos Estudantes')
    plt.xlabel('Nota Final')
    plt.ylabel('Frequ√™ncia')
st.pyplot(plt)
plt.clf()

'''
Com base no histograma, a maioria dos estudantes obteve notas finais elevadas, concentrando-se principalmente na faixa de 70 a 90. H√° uma distribui√ß√£o que parece ser bimodal ou multimodal, com picos not√°veis e uma frequ√™ncia menor de notas mais baixas.
'''

st.write('## Distribui√ß√£o de Atividades por Tipo')
plt.figure(figsize=(10, 6))
# Dicion√°rio de tradu√ß√£o dos tipos de atividades
traducao_atividades = {
    'outcontent': 'Conte√∫do Externo',
    'forumng': 'F√≥rum NG',
    'subpage': 'Subp√°gina',
    'resource': 'Recurso',
    'url': 'URL',
    'homepage': 'P√°gina Inicial',
    'quiz': 'Quiz',
    'ouwiki': 'Wiki da Open University',
    'dataplus': 'DataPlus',
    'glossary': 'Gloss√°rio',
    'htmlactivity': 'Atividade HTML',
    'questionnaire': 'Question√°rio',
    'page': 'P√°gina',
    'folder': 'Pasta',
    '   llaborate': 'Atividades Colaborativas',
    'dualpane': 'Painel Duplo',
    'repeatactivity': 'Atividade Repetida',
    'sharedsubpage': 'Subp√°gina Compartilhada'
}

# Contar atividades √∫nicas por tipo (n√£o estudantes √∫nicos, pois √© sobre atividades)
atividade_counts = merged_df['activity_type'].value_counts()
# Traduzir os √≠ndices (tipos de atividades) - criar novo Series com √≠ndices traduzidos
atividades_traduzidas = [traducao_atividades.get(x, x) for x in atividade_counts.index]
atividade_counts_traduzido = pd.Series(atividade_counts.values, index=atividades_traduzidas)
sns.barplot(x=atividade_counts_traduzido.index, y=atividade_counts_traduzido.values)
plt.title('Distribui√ß√£o de Atividades por Tipo')
plt.xlabel('Tipo de Atividade')
plt.ylabel('N√∫mero de Atividades')
plt.xticks(rotation=45)
st.pyplot(plt)
plt.clf()

'''
A atividade mais realizada √© a 'Conte√∫do Externo' com quase o dobro de execu√ß√µes em rela√ß√£o √† segunda posi√ß√£o que √© 'F√≥rum NG'. A distribui√ß√£o √© acentuadamente desigual, com poucas atividades (como "F√≥rum NG" e "Subp√°gina") tendo uso moderado.
'''


st.markdown('## Explorando valores categ√≥ricos')
## Explorando valores categ√≥ricos
st.dataframe(merged_df.select_dtypes('object').describe().T)

"""
Por meio da an√°lise dos dados categ√≥ricos, os estudantes s√£o, na sua maioria, do g√™nero masculino, at√© 35 anos, que realizaram a atividade do tipo f√≥rum na plataforma e foram aprovados.
"""

st.write('## Distribui√ß√£o de Estudantes por Idade')
plt.figure(figsize=(10, 6))
# Contar estudantes √∫nicos por faixa et√°ria
idade_counts = merged_df.groupby('age_band')['id_student'].nunique()
sns.barplot(x=idade_counts.index, y=idade_counts.values)
plt.title('Distribui√ß√£o de Estudantes por Idade')
plt.xlabel('Faixa Et√°ria')
plt.ylabel('N√∫mero de Estudantes √önicos')
plt.xticks(rotation=45)
st.pyplot(plt)
plt.clf()

'''
Este histograma revela que a maioria dos estudantes se encontra na faixa et√°ria de 35 a 55 anos e a faixa et√°ria dentro do grupo 0-35 √© o segundo maior contingente, enquanto estudantes com mais de 55 anos s√£o a minoria.
'''


st.write('## Distribui√ß√£o de Estudantes por G√™nero')
plt.figure(figsize=(6, 6))
# Contar estudantes √∫nicos por g√™nero
genero_counts = merged_df.groupby('gender')['id_student'].nunique()
sns.barplot(x=genero_counts.index, y=genero_counts.values)
plt.title('Distribui√ß√£o de Estudantes por G√™nero')
plt.xlabel('G√™nero')
plt.ylabel('N√∫mero de Estudantes √önicos')
st.pyplot(plt)
plt.clf()

'''
A diferen√ßa na quantidade entre os g√™neros masculino e feminino √© algo em torno de 60% 
'''

st.write('## Distribui√ß√£o de Estudantes por Regi√£o')
plt.figure(figsize=(10, 6))
# Dicion√°rio de tradu√ß√£o das regi√µes
traducao_regioes = {
    'East Anglian Region': 'Regi√£o de East Anglia',
    'East Midlands Region': 'Regi√£o dos Midlands Orientais',
    'Ireland': 'Irlanda',
    'London Region': 'Regi√£o de Londres',
    'North Region': 'Regi√£o Norte',
    'North East Region': 'Regi√£o Nordeste',
    'North Western Region': 'Regi√£o Noroeste',
    'North West Region': 'Regi√£o Noroeste',
    'Scotland': 'Esc√≥cia',
    'South East Region': 'Regi√£o Sudeste',
    'South Region': 'Regi√£o Sul',
    'South West Region': 'Regi√£o Sudoeste',
    'Wales': 'Pa√≠s de Gales',
    'West Midlands Region': 'Regi√£o dos Midlands Ocidentais',
    'Yorkshire and The Humber Region': 'Regi√£o de Yorkshire e Humber',
    'Yorkshire and the Humber Region': 'Regi√£o de Yorkshire e Humber'
}

# Contar estudantes √∫nicos por regi√£o
regiao_counts = merged_df.groupby('region')['id_student'].nunique().sort_values(ascending=False)
# Traduzir os √≠ndices (regi√µes) - criar novo Series com √≠ndices traduzidos
regioes_traduzidas = [traducao_regioes.get(x, x) for x in regiao_counts.index]
regiao_counts_traduzido = pd.Series(regiao_counts.values, index=regioes_traduzidas)
sns.barplot(x=regiao_counts_traduzido.index, y=regiao_counts_traduzido.values)
plt.title('Distribui√ß√£o de Estudantes por Regi√£o')
plt.xlabel('Regi√£o')
plt.ylabel('N√∫mero de Estudantes √önicos')
plt.xticks(rotation=45)
st.pyplot(plt)
plt.clf()

"""
As regi√µes do sudeste sul det√™m a maior concentra√ß√£o de estudantes, pode ter rela√ß√£o com a presen√ßa de importantes universidades na regi√£o: Universidade de Cambridge, Universidade de Essex, Universidade de Artes de Norwich, entre outras.
A distribui√ß√£o √© relativamente decrescente e sem discrep√¢ncias abruptas.
"""

st.write('## Distribui√ß√£o dos Estudantes por Resultado Final')
plt.figure(figsize=(6, 6))
# Dicion√°rio de tradu√ß√£o dos resultados finais
traducao_resultados = {
    'Pass': 'Aprovado',
    'Distinction': 'Aprova√ß√£o com M√©rito',
    'Withdrawn': 'Desistente',
    'Fail': 'Reprovado'
}

# Contar estudantes √∫nicos por resultado final
resultado_counts = merged_df.groupby('final_result')['id_student'].nunique().sort_values(ascending=False)
# Traduzir os √≠ndices (resultados) - criar novo Series com √≠ndices traduzidos
resultados_traduzidos = [traducao_resultados.get(x, x) for x in resultado_counts.index]
resultado_counts_traduzido = pd.Series(resultado_counts.values, index=resultados_traduzidos)
sns.barplot(x=resultado_counts_traduzido.index, y=resultado_counts_traduzido.values)
plt.title('Distribui√ß√£o dos Estudantes por Resultado Final')
plt.xlabel('Resultado Final')
plt.ylabel('N√∫mero de Estudantes √önicos')
st.pyplot(plt)
plt.clf()

'''
A grande maioria dos estudantes obteve o resultado "Aprovado", superando vastamente as outras categorias. Os resultados de "Aprova√ß√£o com M√©rito", "Desistente" e "Reprovado" representam uma propor√ß√£o muito menor do total de alunos, indicando uma alta taxa de sucesso geral.
'''

st.markdown('## Analisando  a import√¢ncia das classes (feature importance)')

st.markdown("Prepara√ß√£o dos dados para modelos de ML...")
Y = merged_df['final_result']
X = merged_df.loc[:, merged_df.columns != 'final_result']

st.markdown('Removendo as classes irrelevantes ou com alta cardinalidade...')
X = X.drop(['id_student', 'id_site', 'id_assessment', 'code_module', 'code_presentation', 'code_module_y', 'code_module_x'], axis=1)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

@st.cache_data(ttl=7200)  # Cache por 2 horas
def treinar_modelo_oulad(X_train, y_train):
    """Treina o modelo OULAD com cache"""
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import OneHotEncoder
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.impute import SimpleImputer
    import pandas as pd
    
    # Drop rows with NaN in y_train
    nan_rows_train = y_train.isnull()
    X_train_cleaned = X_train[~nan_rows_train].copy()
    y_train_cleaned = y_train[~nan_rows_train].copy()
    
    # Identify categorical and numerical columns
    categorical_cols = X_train_cleaned.select_dtypes(include='object').columns
    numerical_cols = X_train_cleaned.select_dtypes(include=np.number).columns
    
    # Create a column transformer to apply different preprocessing steps to different column types
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', SimpleImputer(strategy='mean'), numerical_cols),
            ('cat', Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='most_frequent')),
                ('onehot', OneHotEncoder(handle_unknown='ignore'))]), categorical_cols)
        ],
        remainder='passthrough' # Keep other columns (numeric) as they are
    )
    
    # Create a pipeline that first preprocesses the data and then trains the model
    ml_model = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', RandomForestClassifier(n_estimators=50, n_jobs=2, max_depth=4, random_state=42))])
    
    # Train the model
    ml_model.fit(X_train_cleaned, y_train_cleaned)
    return ml_model

ml_model = treinar_modelo_oulad(X_train, y_train)

st.markdown("Modelo treinado com sucesso!")
st.markdown("Avaliando do modelo...")

predictions = ml_model.predict(X_test)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Drop rows with NaN in y_test
nan_rows_test = y_test.isnull()
X_test_cleaned = X_test[~nan_rows_test].copy()
y_test_cleaned = y_test[~nan_rows_test].copy()
predictions_cleaned = ml_model.predict(X_test_cleaned)

# Exibir m√©tricas do modelo
st.markdown("### M√©tricas de Avalia√ß√£o do Modelo")

# Calcular m√©tricas individuais
accuracy = accuracy_score(y_test_cleaned, predictions_cleaned)
precision = precision_score(y_test_cleaned, predictions_cleaned, average='weighted', zero_division=0)
recall = recall_score(y_test_cleaned, predictions_cleaned, average='weighted', zero_division=0)
f1 = f1_score(y_test_cleaned, predictions_cleaned, average='weighted', zero_division=0)

# Criar tabela com as m√©tricas
metricas_df = pd.DataFrame({
    'M√©trica': ['Acur√°cia', 'Precis√£o (weighted)', 'Recall (weighted)', 'F1-Score (weighted)'],
    'Valor': [accuracy, precision, recall, f1]
})
metricas_df['Valor'] = metricas_df['Valor'].round(4)
st.dataframe(metricas_df, use_container_width=True, hide_index=True)

from sklearn.inspection import permutation_importance

result = permutation_importance(ml_model, X_test_cleaned, y_test_cleaned, n_repeats=10, random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

# Pegar apenas as top 5 features mais importantes (ordenadas da mais importante para a menos importante)
top_5_idx = sorted_idx[-5:][::-1]  # Reverter para ter a mais importante primeiro
top_5_features = X_test_cleaned.columns[top_5_idx]
top_5_importances = result.importances_mean[top_5_idx]

# Traduzir nomes das vari√°veis para exibi√ß√£o
feature_translation = {
    'date_unregistration': 'Data de cancelamento',
    'date_registration': 'Data de registro',
    'age_band': 'Faixa et√°ria',
    'studied_credits': 'Cr√©ditos cursados',
    'studied_credits_x': 'Cr√©ditos cursados',
    'studied_credits_y': 'Cr√©ditos cursados',
    'score': 'Nota',
    'score_x': 'Nota',
    'score_y': 'Nota',
    'activity_type': 'Tipo de atividade',
    'clicks': 'Cliques',
    'gender': 'G√™nero',
    'region': 'Regi√£o',
    'disability': 'Defici√™ncia',
    'highest_education': 'Escolaridade',
    'imd_band': 'Faixa IMD',
    'num_of_prev_attempts': 'Tentativas anteriores',
    'module_presentation_length': 'Dura√ß√£o do m√≥dulo',
}
top_5_features_pt = [feature_translation.get(f, f) for f in top_5_features]

# Criar gr√°fico de barras
fig, ax = plt.subplots(figsize=(10, 6))
ax.barh(range(len(top_5_features)), top_5_importances)
ax.set_yticks(range(len(top_5_features)))
ax.set_yticklabels(top_5_features_pt)
ax.set_xlabel('Import√¢ncia por Permuta√ß√£o')
ax.set_title('Top 5 Vari√°veis Mais Importantes (OULAD)')
ax.invert_yaxis()  # Mostrar a feature mais importante no topo
fig.tight_layout()
st.pyplot(fig)
st.markdown(
    "Histograma de import√¢ncia das vari√°veis (m√©todo de permuta√ß√£o). "
    "Valores mais altos indicam maior impacto na previs√£o do resultado final."
)
plt.clf()

st.markdown("## Conclus√£o")
st.markdown("Nesta an√°lise explorat√≥ria dos dados do OULAD, conseguimos entender melhor o perfil dos estudantes, suas atividades na plataforma e os fatores que influenciam seu desempenho acad√™mico. Atrav√©s da visualiza√ß√£o dos dados, identificamos padr√µes interessantes, como a predomin√¢ncia de estudantes do g√™nero masculino e a distribui√ß√£o et√°ria dos participantes. Al√©m disso, o treinamento do modelo de aprendizado de m√°quina nos permitiu avaliar a import√¢ncia das diferentes caracter√≠sticas dos dados, destacando quais fatores t√™m maior impacto no resultado final dos estudantes. Essas informa√ß√µes s√£o valiosas para institui√ß√µes educacionais que buscam melhorar a experi√™ncia de aprendizagem e o suporte oferecido aos alunos. Futuras an√°lises podem aprofundar ainda mais esses insights, explorando outras vari√°veis e utilizando t√©cnicas avan√ßadas de modelagem preditiva.")

with open('oulad.pkl', 'wb') as f:
    pickle.dump(ml_model, f)
    f.close()