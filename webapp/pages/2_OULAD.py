from pathlib import Path
import sys

# Adicionar o diret√≥rio webapp ao path do Python
webapp_dir = Path(__file__).parent.parent
if str(webapp_dir) not in sys.path:
    sys.path.insert(0, str(webapp_dir))

import streamlit as st
import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno
import numpy as np
import pickle
from src.openai_interpreter import criar_rodape_sidebar


st.set_page_config(
    page_title="An√°lise Explorat√≥ria dos Dados - OULAD",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded",
)

#st.markdown('# Informa√ß√µes B√°sicas dos Dados do OULAD')
#st.divider()

# Tentar carregar dos pickles primeiro (j√° est√£o no Git LFS)
@st.cache_data(ttl=3600)  # Cache por 1 hora
def carregar_dados_oulad():
    """Carrega dados OULAD, tentando primeiro dos pickles, depois dos CSVs"""
    from src.carregar_dados import carregar_oulad_dados, carregar_dados_oulad_raw
    
    # Tentar carregar do pickle primeiro
    try:
        df = carregar_oulad_dados()
        if df is not None and not df.empty:
            # Se carregou do pickle, retornar como dict para compatibilidade
            return {'oulad_processed': df}
    except Exception as e:
        st.warning(f"N√£o foi poss√≠vel carregar do pickle: {e}")
    
    # Fallback: tentar carregar dos CSVs
    try:
        dataframes_oulad = carregar_dados_oulad_raw()
        return dataframes_oulad
    except FileNotFoundError as e:
        st.error(f"""
        **Erro ao carregar dados OULAD:**
        
        Os arquivos de dados n√£o foram encontrados. Verifique se:
        1. Os arquivos pickle (`oulad_data.pkl` ou `oulad_dataframe.pkl`) est√£o no reposit√≥rio
        2. Os arquivos CSV est√£o em `datasets/oulad_data/`
        
        Erro: {e}
        """)
        st.stop()
    except Exception as e:
        st.error(f"Erro inesperado ao carregar dados: {e}")
        st.stop()

dataframes_oulad = carregar_dados_oulad()

# Se carregou do pickle processado, tentar carregar CSVs tamb√©m para ter DataFrames individuais
if 'oulad_processed' in dataframes_oulad:
    df_processed = dataframes_oulad['oulad_processed']
    st.info("üì¶ Dados carregados do pickle processado. Tentando carregar CSVs individuais para an√°lises detalhadas...")
    
    # Tentar carregar CSVs individuais como fallback
    try:
        from src.carregar_dados import carregar_dados_oulad_raw
        dataframes_oulad_raw = carregar_dados_oulad_raw()
        # Usar os CSVs individuais se dispon√≠veis
        df_assessments = dataframes_oulad_raw.get('assessments', pd.DataFrame()).head(10_000)
        df_courses = dataframes_oulad_raw.get('courses', pd.DataFrame()).head(10_000)
        df_vle = dataframes_oulad_raw.get('vle', pd.DataFrame()).head(10_000)
        df_studentinfo = dataframes_oulad_raw.get('studentInfo', pd.DataFrame()).head(10_000)
        df_studentregistration = dataframes_oulad_raw.get('studentRegistration', pd.DataFrame()).head(10_000)
        df_studentassessment = dataframes_oulad_raw.get('studentAssessment', pd.DataFrame()).head(10_000)
        df_studentvle = dataframes_oulad_raw.get('studentVle', pd.DataFrame()).head(10_000)
        # Atualizar dataframes_oulad para usar os CSVs
        dataframes_oulad = dataframes_oulad_raw
        st.success("‚úÖ CSVs individuais carregados com sucesso!")
    except Exception as e:
        st.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel carregar CSVs individuais: {e}")
        st.warning("Algumas funcionalidades podem estar limitadas. Usando dados processados.")
        # Criar DataFrames vazios para evitar erros
        df_assessments = pd.DataFrame()
        df_courses = pd.DataFrame()
        df_vle = pd.DataFrame()
        df_studentinfo = pd.DataFrame()
        df_studentregistration = pd.DataFrame()
        df_studentassessment = pd.DataFrame()
        df_studentvle = pd.DataFrame()
else:
    # Carregou dos CSVs originais
    df_assessments = dataframes_oulad.get('assessments', pd.DataFrame()).head(10_000)
    df_courses = dataframes_oulad.get('courses', pd.DataFrame()).head(10_000)
    df_vle = dataframes_oulad.get('vle', pd.DataFrame()).head(10_000)
    df_studentinfo = dataframes_oulad.get('studentInfo', pd.DataFrame()).head(10_000)
    df_studentregistration = dataframes_oulad.get('studentRegistration', pd.DataFrame()).head(10_000)
    df_studentassessment = dataframes_oulad.get('studentAssessment', pd.DataFrame()).head(10_000)
    df_studentvle = dataframes_oulad.get('studentVle', pd.DataFrame()).head(10_000)

#function to display basic info for a given dataframe
def show_basic_info(df):
    if df.empty or len(df.columns) == 0:
        print("DataFrame vazio - n√£o √© poss√≠vel exibir informa√ß√µes")
        return
    
    print("========================================================================================================")
    print("HEAD:")
    print(df.head(3))
    print("--------------------------------------------------------------------------------------------------------")
    print("SHAPE:")
    print(df.shape)
    print("--------------------------------------------------------------------------------------------------------")
    print("INFO:")
    print(df.info())
    print("--------------------------------------------------------------------------------------------------------")
    print("DESCRIBE:")
    # Verificar se h√° colunas num√©ricas antes de descrever
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print(df.describe().T.round(2))
    else:
        print("Nenhuma coluna num√©rica para descrever")
    print("--------------------------------------------------------------------------------------------------------")
    print("NULL VALUES:")
    print(df.isnull().sum())
    print("--------------------------------------------------------------------------------------------------------")
    print("UNIQUE VALUES:")
    print(df.nunique())
    print("--------------------------------------------------------------------------------------------------------")
    print("DUPLICATED VALUES:")
    print(df.duplicated().sum())
    print("--------------------------------------------------------------------------------------------------------")
    print("VALUE COUNTS:")
    print(df.select_dtypes(include=['object']).nunique())
    print("--------------------------------------------------------------------------------------------------------")
    print("========================================================================================================")


# st.sidebar.selectbox('Escolha o dataframe para visualizar informa√ß√µes b√°sicas:', 
#              options=list(dataframes_oulad.keys()),
#              key='selected_oulad_dataframe')

# apresentar o dataframe selecionado
#st.dataframe(dataframes_oulad[st.session_state['selected_oulad_dataframe']])

# # apresentar informa√ß√µes b√°sicas do dataframe selecionado
# st.markdown("### Informa√ß√µes b√°sicas do DataFrame:")
# selected_df = dataframes_oulad[st.session_state['selected_oulad_dataframe']]

# st.markdown("#### Head:")
# st.dataframe(selected_df.head(3))

# st.markdown("#### Shape:")
# st.write(selected_df.shape)

# st.markdown("#### Info:")
# # Capture the output of info() to a string
# import io
# buffer = io.StringIO()
# selected_df.info(buf=buffer)
# info_string = buffer.getvalue()

# # Display the string in a st.code block for better formatting
# st.code(info_string, language='text')

# st.markdown("#### Describe:")
# st.dataframe(selected_df.describe().T.round(2))

# st.markdown("#### Null Values:")
# st.write(selected_df.isnull().sum())

# st.markdown("#### Unique Values:")
# st.write(selected_df.nunique())

# st.markdown("#### Duplicated Values:")
# st.write(selected_df.duplicated().sum())

# st.markdown("#### Value Counts (Object Columns):")
# st.write(selected_df.select_dtypes(include=['object']).nunique())

# # Visualiza√ß√£o de dados faltantes usando missingno
# st.markdown("### Visualiza√ß√£o de Dados Faltantes:")
# st.markdown("#### Matriz de Dados Faltantes:")

# fig, ax = plt.subplots()
# msno.matrix(selected_df, figsize=(6, 4), ax=ax)
# st.pyplot(fig)


# Remover colunas apenas se existirem e se o DataFrame n√£o estiver vazio
if not df_vle.empty:
    cols_to_drop_vle = [col for col in ['week_from', 'week_to'] if col in df_vle.columns]
    if cols_to_drop_vle:
        new_vle = df_vle.drop(cols_to_drop_vle, axis=1)
    else:
        new_vle = df_vle.copy()
    
    if not new_vle.empty:
        show_basic_info(new_vle)
else:
    new_vle = pd.DataFrame()
    st.warning("‚ö†Ô∏è DataFrame VLE est√° vazio. Algumas an√°lises podem n√£o estar dispon√≠veis.")

# Imputa√ß√£o com os valores mais frequentes por regi√£o (apenas se studentInfo n√£o estiver vazio)
if not df_studentinfo.empty and 'imd_band' in df_studentinfo.columns and 'region' in df_studentinfo.columns:
    if 'studentInfo' in dataframes_oulad and not dataframes_oulad['studentInfo'].empty:
        dataframes_oulad['studentInfo']['imd_band_2'] = dataframes_oulad['studentInfo'].apply(
            lambda x: dataframes_oulad['studentInfo'][dataframes_oulad['studentInfo']['region']==x['region']]['imd_band'].mode()[0] 
            if pd.isna(x['imd_band']) and len(dataframes_oulad['studentInfo'][dataframes_oulad['studentInfo']['region']==x['region']]['imd_band'].mode()) > 0 
            else x['imd_band'], axis=1
        )
    
    # Remover coluna apenas se existir
    if 'imd_band' in df_studentinfo.columns:
        new_studentInfo = df_studentinfo.drop(['imd_band'], axis=1)
    else:
        new_studentInfo = df_studentinfo.copy()
else:
    new_studentInfo = df_studentinfo.copy()
show_basic_info(new_studentInfo)

# Imputando valores ausentes em 'date_registration' e 'date_unregistration'
# Criar uma c√≥pia expl√≠cita do dataframe para evitar SettingWithCopyWarning
df_student_registration_copy = df_studentregistration.copy()

# Verificar se o DataFrame n√£o est√° vazio e se as colunas existem antes de process√°-las
if not df_student_registration_copy.empty and 'date_unregistration' in df_student_registration_copy.columns:
    # Criar vari√°vel bin√°ria indicando se o estudante cancelou o registro
    df_student_registration_copy['cancelou'] = df_student_registration_copy['date_unregistration'].notna().astype(int)
    
    # Preencher date_unregistration com valor alto quando ausente (para diferenciar de valores reais)
    # Usar max + 1000 para garantir que seja claramente distinto de qualquer data real
    max_date_unregistration = df_student_registration_copy['date_unregistration'].max()
    if pd.notna(max_date_unregistration):
        valor_nao_cancelou = max_date_unregistration + 1000
    else:
        # Se todos os valores forem NaN, usar um valor padr√£o alto
        valor_nao_cancelou = 999999
    df_student_registration_copy['date_unregistration'] = df_student_registration_copy['date_unregistration'].fillna(valor_nao_cancelou)
else:
    # Se a coluna n√£o existir ou o DataFrame estiver vazio, criar uma coluna 'cancelou' com valores padr√£o
    if not df_student_registration_copy.empty:
        df_student_registration_copy['cancelou'] = 0
        st.warning("‚ö†Ô∏è Coluna 'date_unregistration' n√£o encontrada. Usando valores padr√£o para 'cancelou'.")
    else:
        st.warning("‚ö†Ô∏è DataFrame 'studentRegistration' est√° vazio. Pulando processamento de datas de registro.")

# Preencher date_registration com a m√©dia quando ausente
if not df_student_registration_copy.empty and 'date_registration' in df_student_registration_copy.columns:
    mean_date_registration = df_student_registration_copy['date_registration'].mean()
    df_student_registration_copy['date_registration'] = df_student_registration_copy['date_registration'].fillna(mean_date_registration)
else:
    st.warning("‚ö†Ô∏è Coluna 'date_registration' n√£o encontrada.")

# Jun√ß√£o dos dados (apenas se os DataFrames necess√°rios n√£o estiverem vazios)
@st.cache_data(ttl=3600)  # Cache por 1 hora
def merge_dataframes():
    if df_studentvle.empty or new_vle.empty:
        st.warning("‚ö†Ô∏è DataFrames VLE est√£o vazios. Pulando merge de VLE.")
        vle_activities = df_studentvle.copy()
    else:
        try:
            vle_activities = pd.merge(df_studentvle, new_vle, on=['code_module','code_presentation','id_site'], how='inner')
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erro ao fazer merge de VLE: {e}")
            vle_activities = df_studentvle.copy()
    
    if df_studentassessment.empty or df_assessments.empty:
        st.warning("‚ö†Ô∏è DataFrames de assessments est√£o vazios. Pulando merge de assessments.")
        assessments_activities = df_studentassessment.copy()
    else:
        try:
            assessments_activities = pd.merge(df_studentassessment, df_assessments, on='id_assessment', how='inner')
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erro ao fazer merge de assessments: {e}")
            assessments_activities = df_studentassessment.copy()
    
    if vle_activities.empty or new_studentInfo.empty:
        st.warning("‚ö†Ô∏è DataFrames necess√°rios para merge est√£o vazios. Pulando merge com studentInfo.")
        studentinfo_activities = vle_activities.copy()
    else:
        try:
            studentinfo_activities = pd.merge(vle_activities, new_studentInfo, on=['code_module','code_presentation','id_student'], how='inner')
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erro ao fazer merge com studentInfo: {e}")
            studentinfo_activities = vle_activities.copy()
    
    if studentinfo_activities.empty or assessments_activities.empty:
        st.warning("‚ö†Ô∏è DataFrames necess√°rios para merge final est√£o vazios.")
        merged_df = studentinfo_activities.copy()
    else:
        try:
            merged_df = pd.merge(studentinfo_activities, assessments_activities, on=['code_module','code_presentation','id_student'], how='inner')
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erro ao fazer merge final: {e}")
            merged_df = studentinfo_activities.copy()
    
    return merged_df

merged_df = merge_dataframes()
st.session_state['merged_df'] = merged_df

# Merge with courses dataframe (apenas se n√£o estiver vazio)
if not merged_df.empty and not df_courses.empty:
    try:
        merged_df = pd.merge(merged_df, df_courses, on=['code_presentation'], how='inner')
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Erro ao fazer merge com courses: {e}")
elif df_courses.empty:
    st.warning("‚ö†Ô∏è DataFrame courses est√° vazio. Pulando merge com courses.")

# Merge with studentRegistration dataframe (usando a vers√£o processada com vari√°vel cancelou)
if not merged_df.empty and not df_student_registration_copy.empty:
    try:
        merged_df = pd.merge(merged_df, df_student_registration_copy, on=['code_presentation','id_student'], how='inner')
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Erro ao fazer merge com studentRegistration: {e}")
elif df_student_registration_copy.empty:
    st.warning("‚ö†Ô∏è DataFrame studentRegistration est√° vazio. Pulando merge com studentRegistration.")

# Imputing missing values for numerical columns with the mean
for col in merged_df.select_dtypes(include=np.number).columns:
    merged_df[col].fillna(merged_df[col].mean(), inplace=True)

# Imputing missing values for categorical columns with the most frequent value
for col in merged_df.select_dtypes(include='object').columns:
    merged_df[col].fillna(merged_df[col].mode()[0], inplace=True)

# st.write("Merged DataFrame after handling missing values:")
# st.dataframe(merged_df.isnull().sum())

# Sidebar com rodap√©
with st.sidebar:
    st.markdown("### üìä Informa√ß√µes")
    st.info("""
    Esta p√°gina apresenta uma an√°lise explorat√≥ria dos dados do OULAD (Open University Learning Analytics Dataset).
    """)
    st.markdown("---")
    # Rodap√© com badges de status (igual ao da home)
    criar_rodape_sidebar()

st.write('# An√°lise Explorat√≥ria de Dados (EDA) - OULAD')

'''
Esta p√°gina apresenta uma An√°lise Explorat√≥ria dos Dados do OULAD (Open University Learning Analytics Dataset), com foco em entender o perfil dos estudantes, suas atividades na plataforma e fatores que influenciam o desempenho acad√™mico . Atrav√©s de visualiza√ß√µes, s√£o identificados padr√µes relevantes, como a predomin√¢ncia de estudantes do g√™nero masculino e a distribui√ß√£o et√°ria dos estudantes.
'''

st.markdown("## Descri√ß√£o estat√≠sticas das colunas num√©ricas:")
numeric_df = merged_df.select_dtypes('number')
if not numeric_df.empty and len(numeric_df.columns) > 0:
    st.dataframe(numeric_df.describe().T.round(2))
else:
    st.warning("‚ö†Ô∏è N√£o h√° colunas num√©ricas dispon√≠veis para an√°lise estat√≠stica.")

'''
A grande diferen√ßa entre a mediana (‚âà2) e a m√©dia (‚âà4.65) do n√∫mero de cliques indica que a maioria dos estudantes tem engajamento moderado, mas uma pequena parcela √© extremamente ativa, elevando a m√©dia geral.

O n√∫mero de tentativas anteriores √© zero para a vasta maioria dos estudantes (quartis e valor m√°ximo s√£o 0), sugerindo que o conjunto de dados est√° focado na performance na primeira tentativa.
'''


st.write('## Distribui√ß√£o das notas finais dos estudantes')
plt.figure(figsize=(10, 6))
# Calcular nota m√©dia por estudante √∫nico
if 'score' in merged_df.columns and 'id_student' in merged_df.columns:
    notas_por_estudante = merged_df.groupby('id_student')['score'].mean()
    sns.histplot(notas_por_estudante, bins=30, kde=True)
    plt.title('Distribui√ß√£o de Notas Finais dos Estudantes (√önicos)')
    plt.xlabel('Nota Final M√©dia')
    plt.ylabel('N√∫mero de Estudantes √önicos')
    st.pyplot(plt)
    plt.clf()
elif 'score' in merged_df.columns:
    sns.histplot(merged_df['score'], bins=30, kde=True)
    plt.title('Distribui√ß√£o de Notas Finais dos Estudantes')
    plt.xlabel('Nota Final')
    plt.ylabel('Frequ√™ncia')
    st.pyplot(plt)
    plt.clf()
else:
    st.warning("‚ö†Ô∏è Coluna 'score' n√£o encontrada nos dados. N√£o √© poss√≠vel exibir a distribui√ß√£o de notas.")

'''
Com base no histograma, a maioria dos estudantes obteve notas finais elevadas, concentrando-se principalmente na faixa de 70 a 90. H√° uma distribui√ß√£o que parece ser bimodal ou multimodal, com picos not√°veis e uma frequ√™ncia menor de notas mais baixas.
'''

st.write('## Distribui√ß√£o de Atividades por Tipo')
plt.figure(figsize=(10, 6))
# Dicion√°rio de tradu√ß√£o dos tipos de atividades
traducao_atividades = {
    'outcontent': 'Conte√∫do Externo',
    'forumng': 'F√≥rum NG',
    'subpage': 'Subp√°gina',
    'resource': 'Recurso',
    'url': 'URL',
    'homepage': 'P√°gina Inicial',
    'quiz': 'Quiz',
    'ouwiki': 'Wiki da Open University',
    'dataplus': 'DataPlus',
    'glossary': 'Gloss√°rio',
    'htmlactivity': 'Atividade HTML',
    'questionnaire': 'Question√°rio',
    'page': 'P√°gina',
    'folder': 'Pasta',
    '   llaborate': 'Atividades Colaborativas',
    'dualpane': 'Painel Duplo',
    'repeatactivity': 'Atividade Repetida',
    'sharedsubpage': 'Subp√°gina Compartilhada'
}

# Contar atividades √∫nicas por tipo (n√£o estudantes √∫nicos, pois √© sobre atividades)
atividade_counts = merged_df['activity_type'].value_counts()
# Traduzir os √≠ndices (tipos de atividades) - criar novo Series com √≠ndices traduzidos
atividades_traduzidas = [traducao_atividades.get(x, x) for x in atividade_counts.index]
atividade_counts_traduzido = pd.Series(atividade_counts.values, index=atividades_traduzidas)
sns.barplot(x=atividade_counts_traduzido.index, y=atividade_counts_traduzido.values)
plt.title('Distribui√ß√£o de Atividades por Tipo')
plt.xlabel('Tipo de Atividade')
plt.ylabel('N√∫mero de Atividades')
plt.xticks(rotation=45)
st.pyplot(plt)
plt.clf()

'''
A atividade mais realizada √© a 'Conte√∫do Externo' com quase o dobro de execu√ß√µes em rela√ß√£o √† segunda posi√ß√£o que √© 'F√≥rum NG'. A distribui√ß√£o √© acentuadamente desigual, com poucas atividades (como "F√≥rum NG" e "Subp√°gina") tendo uso moderado.
'''


st.markdown('## Explorando valores categ√≥ricos')
## Explorando valores categ√≥ricos
categorical_df = merged_df.select_dtypes('object')
if not categorical_df.empty and len(categorical_df.columns) > 0:
    st.dataframe(categorical_df.describe().T)
else:
    st.warning("‚ö†Ô∏è N√£o h√° colunas categ√≥ricas dispon√≠veis para an√°lise.")

"""
Por meio da an√°lise dos dados categ√≥ricos, os estudantes s√£o, na sua maioria, do g√™nero masculino, at√© 35 anos, que realizaram a atividade do tipo f√≥rum na plataforma e foram aprovados.
"""

col1, col2 = st.columns(2)

with col1:
    st.write('## Distribui√ß√£o de Estudantes por Idade')
    # Contar estudantes √∫nicos por faixa et√°ria
    idade_counts = merged_df.groupby('age_band')['id_student'].nunique()
    fig_idade, ax_idade = plt.subplots(figsize=(6, 4))
    sns.barplot(x=idade_counts.index, y=idade_counts.values, ax=ax_idade)
    ax_idade.set_title('Distribui√ß√£o de Estudantes por Idade')
    ax_idade.set_xlabel('Faixa Et√°ria')
    ax_idade.set_ylabel('N√∫mero de Estudantes √önicos')
    ax_idade.tick_params(axis='x', rotation=45)
    st.pyplot(fig_idade)

    '''
    Este histograma revela que a maioria dos estudantes se encontra na faixa et√°ria de 35 a 55 anos e a faixa et√°ria dentro do grupo 0-35 √© o segundo maior contingente, enquanto estudantes com mais de 55 anos s√£o a minoria.
    '''

with col2:
    st.write('## Distribui√ß√£o de Estudantes por G√™nero')
    # Contar estudantes √∫nicos por g√™nero
    genero_counts = merged_df.groupby('gender')['id_student'].nunique()
    fig_genero, ax_genero = plt.subplots(figsize=(6, 4))
    sns.barplot(x=genero_counts.index, y=genero_counts.values, ax=ax_genero)
    ax_genero.set_title('Distribui√ß√£o de Estudantes por G√™nero')
    ax_genero.set_xlabel('G√™nero')
    ax_genero.set_ylabel('N√∫mero de Estudantes √önicos')
    st.pyplot(fig_genero)

    # Espa√ßo extra para alinhar com o texto do gr√°fico ao lado
    st.markdown("<div style='margin-top:8px'></div>", unsafe_allow_html=True)

    '''
    A diferen√ßa na quantidade entre os g√™neros masculino e feminino √© algo em torno de 60% 
    '''

st.write('## Distribui√ß√£o de Estudantes por Regi√£o')
plt.figure(figsize=(10, 6))
# Dicion√°rio de tradu√ß√£o das regi√µes
traducao_regioes = {
    'East Anglian Region': 'Regi√£o de East Anglia',
    'East Midlands Region': 'Regi√£o dos Midlands Orientais',
    'Ireland': 'Irlanda',
    'London Region': 'Regi√£o de Londres',
    'North Region': 'Regi√£o Norte',
    'North East Region': 'Regi√£o Nordeste',
    'North Western Region': 'Regi√£o Noroeste',
    'North West Region': 'Regi√£o Noroeste',
    'Scotland': 'Esc√≥cia',
    'South East Region': 'Regi√£o Sudeste',
    'South Region': 'Regi√£o Sul',
    'South West Region': 'Regi√£o Sudoeste',
    'Wales': 'Pa√≠s de Gales',
    'West Midlands Region': 'Regi√£o dos Midlands Ocidentais',
    'Yorkshire and The Humber Region': 'Regi√£o de Yorkshire e Humber',
    'Yorkshire and the Humber Region': 'Regi√£o de Yorkshire e Humber'
}

# Contar estudantes √∫nicos por regi√£o
regiao_counts = merged_df.groupby('region')['id_student'].nunique().sort_values(ascending=False)
# Traduzir os √≠ndices (regi√µes) - criar novo Series com √≠ndices traduzidos
regioes_traduzidas = [traducao_regioes.get(x, x) for x in regiao_counts.index]
regiao_counts_traduzido = pd.Series(regiao_counts.values, index=regioes_traduzidas)
sns.barplot(x=regiao_counts_traduzido.index, y=regiao_counts_traduzido.values)
plt.title('Distribui√ß√£o de Estudantes por Regi√£o')
plt.xlabel('Regi√£o')
plt.ylabel('N√∫mero de Estudantes √önicos')
plt.xticks(rotation=45)
st.pyplot(plt)
plt.clf()

"""
As regi√µes do sudeste sul det√™m a maior concentra√ß√£o de estudantes, pode ter rela√ß√£o com a presen√ßa de importantes universidades na regi√£o: Universidade de Cambridge, Universidade de Essex, Universidade de Artes de Norwich, entre outras.
A distribui√ß√£o √© relativamente decrescente e sem discrep√¢ncias abruptas.
"""

st.write('## Distribui√ß√£o dos Estudantes por Resultado Final')
# Tamanho reduzido para evitar ocupar toda a largura
fig_resultado, ax_resultado = plt.subplots(figsize=(6, 4))
# Dicion√°rio de tradu√ß√£o dos resultados finais
traducao_resultados = {
    'Pass': 'Aprovado',
    'Distinction': 'Aprova√ß√£o com M√©rito',
    'Withdrawn': 'Desistente',
    'Fail': 'Reprovado'
}

# Contar estudantes √∫nicos por resultado final
resultado_counts = merged_df.groupby('final_result')['id_student'].nunique().sort_values(ascending=False)
# Traduzir os √≠ndices (resultados) - criar novo Series com √≠ndices traduzidos
resultados_traduzidos = [traducao_resultados.get(x, x) for x in resultado_counts.index]
resultado_counts_traduzido = pd.Series(resultado_counts.values, index=resultados_traduzidos)
sns.barplot(x=resultado_counts_traduzido.index, y=resultado_counts_traduzido.values, ax=ax_resultado)
ax_resultado.set_title('Distribui√ß√£o dos Estudantes por Resultado Final')
ax_resultado.set_xlabel('Resultado Final')
ax_resultado.set_ylabel('N√∫mero de Estudantes √önicos')
st.pyplot(fig_resultado)

'''
A grande maioria dos estudantes obteve o resultado "Aprovado", superando vastamente as outras categorias. Os resultados de "Aprova√ß√£o com M√©rito", "Desistente" e "Reprovado" representam uma propor√ß√£o muito menor do total de alunos, indicando uma alta taxa de sucesso geral.
'''

st.markdown('## Analisando  a import√¢ncia das classes (feature importance)')

st.markdown("Prepara√ß√£o dos dados para modelos de ML...")
Y = merged_df['final_result']
X = merged_df.loc[:, merged_df.columns != 'final_result']

st.markdown('Removendo as classes irrelevantes ou com alta cardinalidade...')
X = X.drop(['id_student', 'id_site', 'id_assessment', 'code_module', 'code_presentation', 'code_module_y', 'code_module_x'], axis=1)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

@st.cache_data(ttl=7200)  # Cache por 2 horas
def treinar_modelo_oulad(X_train, y_train):
    """Treina o modelo OULAD com cache"""
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import OneHotEncoder
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.impute import SimpleImputer
    import pandas as pd
    
    # Drop rows with NaN in y_train
    nan_rows_train = y_train.isnull()
    X_train_cleaned = X_train[~nan_rows_train].copy()
    y_train_cleaned = y_train[~nan_rows_train].copy()
    
    # Identificar colunas categ√≥ricas de forma mais robusta
    # Incluir 'object', 'category' e verificar colunas que cont√™m strings
    categorical_cols = []
    numerical_cols = []
    
    # Lista de colunas conhecidas como categ√≥ricas no OULAD
    known_categorical = ['activity_type', 'gender', 'region', 'highest_education', 
                        'imd_band', 'age_band', 'disability', 'code_module', 
                        'code_presentation', 'assessment_type']
    
    for col in X_train_cleaned.columns:
        # Verificar se √© num√©rico puro
        if pd.api.types.is_numeric_dtype(X_train_cleaned[col]):
            # Verificar se n√£o cont√©m strings (pode ter sido convertido incorretamente)
            try:
                pd.to_numeric(X_train_cleaned[col], errors='raise')
                numerical_cols.append(col)
            except (ValueError, TypeError):
                # Se n√£o pode ser convertido para num√©rico, √© categ√≥rico
                categorical_cols.append(col)
        else:
            # √â categ√≥rico (object, category, ou string)
            categorical_cols.append(col)
    
    # Garantir que colunas conhecidas como categ√≥ricas estejam na lista
    for col in known_categorical:
        if col in X_train_cleaned.columns and col not in categorical_cols:
            categorical_cols.append(col)
            if col in numerical_cols:
                numerical_cols.remove(col)
    
    # Converter todas as colunas categ√≥ricas para string explicitamente
    for col in categorical_cols:
        if col in X_train_cleaned.columns:
            X_train_cleaned[col] = X_train_cleaned[col].astype(str)
            # Substituir 'nan' string por np.nan
            X_train_cleaned[col] = X_train_cleaned[col].replace('nan', np.nan)
            X_train_cleaned[col] = X_train_cleaned[col].replace('None', np.nan)
    
    # Converter colunas num√©ricas para float, tratando inf
    for col in numerical_cols:
        if col in X_train_cleaned.columns:
            X_train_cleaned[col] = pd.to_numeric(X_train_cleaned[col], errors='coerce')
            # Substituir inf por NaN
            X_train_cleaned[col] = X_train_cleaned[col].replace([np.inf, -np.inf], np.nan)
    
    # Remover colunas que ficaram vazias ap√≥s limpeza
    cols_to_drop = []
    for col in X_train_cleaned.columns:
        if X_train_cleaned[col].isna().all():
            cols_to_drop.append(col)
    
    if cols_to_drop:
        X_train_cleaned = X_train_cleaned.drop(columns=cols_to_drop)
        categorical_cols = [c for c in categorical_cols if c not in cols_to_drop]
        numerical_cols = [c for c in numerical_cols if c not in cols_to_drop]
    
    # Garantir que temos pelo menos algumas colunas
    if len(categorical_cols) == 0 and len(numerical_cols) == 0:
        raise ValueError("Nenhuma coluna v√°lida encontrada ap√≥s limpeza dos dados")
    
    # Criar transformers apenas para colunas que existem
    transformers = []
    if len(numerical_cols) > 0:
        transformers.append(('num', SimpleImputer(strategy='mean'), numerical_cols))
    if len(categorical_cols) > 0:
        transformers.append(('cat', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), categorical_cols))
    
    # Create a column transformer to apply different preprocessing steps to different column types
    # Usar 'drop' em vez de 'passthrough' para garantir que todas as colunas sejam processadas
    preprocessor = ColumnTransformer(
        transformers=transformers,
        remainder='drop'  # Drop any columns not explicitly handled
    )
    
    # Create a pipeline that first preprocesses the data and then trains the model
    ml_model = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', RandomForestClassifier(n_estimators=50, n_jobs=2, max_depth=4, random_state=42))])
    
    # Train the model
    ml_model.fit(X_train_cleaned, y_train_cleaned)
    return ml_model

ml_model = treinar_modelo_oulad(X_train, y_train)

st.markdown("Modelo treinado com sucesso!")
st.markdown("Avaliando do modelo...")

predictions = ml_model.predict(X_test)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Drop rows with NaN in y_test
nan_rows_test = y_test.isnull()
X_test_cleaned = X_test[~nan_rows_test].copy()
y_test_cleaned = y_test[~nan_rows_test].copy()
predictions_cleaned = ml_model.predict(X_test_cleaned)

# Exibir m√©tricas do modelo
st.markdown("### M√©tricas de Avalia√ß√£o do Modelo")

# Calcular m√©tricas individuais
accuracy = accuracy_score(y_test_cleaned, predictions_cleaned)
precision = precision_score(y_test_cleaned, predictions_cleaned, average='weighted', zero_division=0)
recall = recall_score(y_test_cleaned, predictions_cleaned, average='weighted', zero_division=0)
f1 = f1_score(y_test_cleaned, predictions_cleaned, average='weighted', zero_division=0)

# Criar tabela com as m√©tricas
metricas_df = pd.DataFrame({
    'M√©trica': ['Acur√°cia', 'Precis√£o (weighted)', 'Recall (weighted)', 'F1-Score (weighted)'],
    'Valor': [accuracy, precision, recall, f1]
})
metricas_df['Valor'] = metricas_df['Valor'].round(4)
st.dataframe(metricas_df, use_container_width=True, hide_index=True)

from sklearn.inspection import permutation_importance

result = permutation_importance(ml_model, X_test_cleaned, y_test_cleaned, n_repeats=10, random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

# Pegar apenas as top 5 features mais importantes (ordenadas da mais importante para a menos importante)
top_5_idx = sorted_idx[-5:][::-1]  # Reverter para ter a mais importante primeiro
top_5_features = X_test_cleaned.columns[top_5_idx]
top_5_importances = result.importances_mean[top_5_idx]

# Traduzir nomes das vari√°veis para exibi√ß√£o
feature_translation = {
    'date_unregistration': 'Data de cancelamento',
    'date_registration': 'Data de registro',
    'age_band': 'Faixa et√°ria',
    'studied_credits': 'Cr√©ditos cursados',
    'studied_credits_x': 'Cr√©ditos cursados',
    'studied_credits_y': 'Cr√©ditos cursados',
    'score': 'Nota',
    'score_x': 'Nota',
    'score_y': 'Nota',
    'activity_type': 'Tipo de atividade',
    'clicks': 'Cliques',
    'gender': 'G√™nero',
    'region': 'Regi√£o',
    'disability': 'Defici√™ncia',
    'highest_education': 'Escolaridade',
    'imd_band': 'Faixa IMD',
    'num_of_prev_attempts': 'Tentativas anteriores',
    'module_presentation_length': 'Dura√ß√£o do m√≥dulo',
    'cancelou': 'Cancelou',
}
top_5_features_pt = [feature_translation.get(f, f) for f in top_5_features]

# Criar gr√°fico de barras
fig, ax = plt.subplots(figsize=(10, 6))
ax.barh(range(len(top_5_features)), top_5_importances)
ax.set_yticks(range(len(top_5_features)))
ax.set_yticklabels(top_5_features_pt)
ax.set_xlabel('Import√¢ncia por Permuta√ß√£o')
ax.set_title('Top 5 Vari√°veis Mais Importantes (OULAD)')
ax.invert_yaxis()  # Mostrar a feature mais importante no topo
fig.tight_layout()
st.pyplot(fig)
st.markdown(
    "Histograma de import√¢ncia das vari√°veis (m√©todo de permuta√ß√£o). "
    "Valores mais altos indicam maior impacto na previs√£o do resultado final."
)
plt.clf()

st.markdown("## Conclus√£o")
st.markdown("Nesta an√°lise explorat√≥ria dos dados do OULAD, conseguimos entender melhor o perfil dos estudantes, suas atividades na plataforma e os fatores que influenciam seu desempenho acad√™mico. Atrav√©s da visualiza√ß√£o dos dados, identificamos padr√µes interessantes, como a predomin√¢ncia de estudantes do g√™nero masculino e a distribui√ß√£o et√°ria dos participantes. Al√©m disso, o treinamento do modelo de aprendizado de m√°quina nos permitiu avaliar a import√¢ncia das diferentes caracter√≠sticas dos dados, destacando quais fatores t√™m maior impacto no resultado final dos estudantes. Essas informa√ß√µes s√£o valiosas para institui√ß√µes educacionais que buscam melhorar a experi√™ncia de aprendizagem e o suporte oferecido aos alunos. Futuras an√°lises podem aprofundar ainda mais esses insights, explorando outras vari√°veis e utilizando t√©cnicas avan√ßadas de modelagem preditiva.")

with open('oulad.pkl', 'wb') as f:
    pickle.dump(ml_model, f)
    f.close()